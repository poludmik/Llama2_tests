{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaModel # LlamaModel is without language head, raw hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1743: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"from_hf/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference 7B model (predict continuation of a prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.17s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"llama-2-7b_bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is wrong with the world? Why is there so much suffering? Why is there so much injustice? Why is there so much pain'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is wrong with\"\n",
    "# tokens = torch.tensor([tokenizer.encode(text)])\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "# print(model.forward(input_ids=tokens, output_hidden_states=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat verstion on 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-chat_bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.84s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"llama-2-7b-chat_bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: A rap battle between Stephen Colbert and John Oliver\\n\\nStephen Colbert:\\nYo, John, you're a Brit, but you're trying to be cool\\nLike a hip-hop star, but\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Question: A rap battle between Stephen Colbert and John Oliver\"\n",
    "# tokens = torch.tensor([tokenizer.encode(text)])\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "# print(model.forward(input_ids=tokens, output_hidden_states=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load shared library '/home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/libllama.so': /opt/apps/software/Anaconda3/2021.11/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/libllama.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:67\u001b[0m, in \u001b[0;36m_load_shared_library\u001b[0;34m(lib_base_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m ctypes\u001b[39m.\u001b[39;49mCDLL(\u001b[39mstr\u001b[39;49m(_lib_path), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcdll_args)\n\u001b[1;32m     68\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/apps/software/Anaconda3/2021.11/lib/python3.9/ctypes/__init__.py:382\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: /opt/apps/software/Anaconda3/2021.11/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/libllama.so)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from langchain.embeddings import LlamaCppEmbeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mllama\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:80\u001b[0m\n\u001b[1;32m     77\u001b[0m _lib_base_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllama\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39m# Load the library\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m _lib \u001b[39m=\u001b[39m _load_shared_library(_lib_base_name)\n\u001b[1;32m     82\u001b[0m \u001b[39m# Misc\u001b[39;00m\n\u001b[1;32m     83\u001b[0m c_float_p \u001b[39m=\u001b[39m POINTER(c_float)\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:69\u001b[0m, in \u001b[0;36m_load_shared_library\u001b[0;34m(lib_base_name)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[39mreturn\u001b[39;00m ctypes\u001b[39m.\u001b[39mCDLL(\u001b[39mstr\u001b[39m(_lib_path), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcdll_args)\n\u001b[1;32m     68\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 69\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load shared library \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m_lib_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShared library with base name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlib_base_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load shared library '/home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/libllama.so': /opt/apps/software/Anaconda3/2021.11/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/poludmik/virtual_env_project/my_venv/lib/python3.9/site-packages/llama_cpp/libllama.so)"
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings import LlamaCppEmbeddings\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/langchain/embeddings/llamacpp.py:84\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[1;32m     86\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llama \u001b[39m=\u001b[39m LlamaCppEmbeddings(model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllama-2-7b-chat_bin/\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# \u001b[39;00m\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/virtual_env_project/my_venv/lib/python3.9/site-packages/langchain/embeddings/llamacpp.py:88\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     86\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Llama(model_path, embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n\u001b[1;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama-cpp-python library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install the llama-cpp-python library to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     95\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load Llama model from path: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "llama = LlamaCppEmbeddings(model_path=\"llama-2-7b-chat_bin/\") # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
